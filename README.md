# Document Streaming Application
Data Streaming workflow that sends JSON data via an API, and processed downstream with Apache Kafka and Apache Spark streaming, and persisted in a MongoDB instance.

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/doc-streaming-banner.png)

---

## Application Architecture

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/pipeline-architecture.png)

---

## Project Setup

---

### Data Preparation

---
### Build and deploy the Data API

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/fastapi-ui.png)


---

### Set up and deploy Kafka

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/testing-sparkoutput-consumption.png)


---

### Test API connectivity using Insomnia


---

### Setup and Deploy Spark Structured Streaming

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/spark-ui-01.png)

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/spark-ui-01.png)

[!image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/testing-sparkoutput-consumption.png)

### Deploy MongoDB and Mongo Express

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/mongo-express-ui.png)

### Create Dashboard UI with Streamlit

![image](https://github.com/ovokpus/document-streaming-pipeline/blob/main/img/streamlit%20dashboard.png)

## Ideas for improvement and tradeoffs
